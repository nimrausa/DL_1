# -*- coding: utf-8 -*-
"""DL Project 1. Breast Cancer Classification with Neural Network .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CXnb3yenHA5v_13HOzpNFYorGqz-iS--
"""

import numpy as np
import pandas as pd
import sklearn.datasets 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

"""Importing the Dependencies

Data collection and processing
"""

# loading the data from sklearn (we are not reading the file therefore pandas_read.csv is not used )
breast_cancer_dataset=sklearn.datasets.load_breast_cancer()
print(breast_cancer_dataset)

# loading the data into data frame using pandas
data_frame = pd.DataFrame(breast_cancer_dataset.data, columns=breast_cancer_dataset.feature_names)

#print the first five rows of dataframe
data_frame.head()

# adding the 'target' column to the data frame
data_frame['label'] = breast_cancer_dataset.target

#print the first five rows of dataframe
data_frame.tail()

# number of rows and columns in the datafreame
data_frame.shape

#getting the information about data
data_frame.info()

#checking for missing values
data_frame.isnull().sum()

# stastical measures of the data set
data_frame.describe()

#ckecking the distribution of the target variables
data_frame['label'].value_counts()

"""1===Benign
0===Malignant
"""

data_frame.groupby('label').mean()

"""Separating the features and target"""

X=data_frame.drop(columns='label', axis=1)
Y = data_frame['label']

print(X)

print(Y)

"""Splitting the data into training data and test data"""

X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.2, random_state=2)

print(X.shape, X_train.shape , X_test.shape)

"""Standarizing the data"""

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X_train_std=scaler.fit_transform(X_train)
X_test_std=scaler.fit_transform(X_test)

"""Importing"""

import tensorflow as tf
tf.random.set_seed(3)# to get a constant value and produce reproducible results
from tensorflow import keras

from tensorflow.python.framework.tensor_spec import DenseSpec
model=keras.Sequential([
                        keras.layers.Flatten(input_shape=(30,)),#input 30--no. of columns except target only features.Flatten convert this into single dimensional array
                        keras.layers.Dense(20,activation='relu'),#hidden--to add neural network
                        keras.layers.Dense(2,activation='sigmoid')# 2--output neurons=no. of classes
                       ])

#compiling the neural network
from sklearn import metrics
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy', #if we have target 0 or 1
              metrics=['accuracy'])

#training the neural network
history=model.fit(X_train_std,Y_train,validation_split=0.1,epochs=10)
#validation_split=0.1=test size
#epochs=10--how many times modelneed to go through the data

"""Visualizing accuracy and loss"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['training data','validation data'], loc='upper right')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['training data','validation data'], loc='upper right')

"""Accuracy of the model on test data"""

loss, accuracy=model.evaluate(X_test_std,Y_test)

print(accuracy)

print(X_test_std.shape)
print(X_test_std[0])# first data point in the data det

Y_pred=model.predict(X_test_std)
print(Y_pred.shape)
print(Y_pred[0])

print(X_test_std)

Y_pred_labels=[np.argmax(i) for i in Y_pred]
print(Y_pred_labels)

"""Building the predictive system"""

input_data=(13.03,18.42,82.61,523.8,0.08983,0.03766,0.02562,0.02923,0.1467,0.05863,0.1839,2.342,1.17,14.16,0.004352,0.004899,0.01343,0.01164,0.02671,0.001777,13.3,22.81,84.46,545.9,0.09701,0.04619,0.04833,0.05013,0.1987,0.06169)
#change the input data to the numpy array
input_data_as_numpy_array=np.asarray(input_data)

#reshape the numpy array as we are predicting for one data point
input_data_as_reshaped=input_data_as_numpy_array.reshape(1,-1)

#strandardizing the input dta
input_data_std=scaler.transform(input_data_as_reshaped)
prediction=model.predict(input_data_std)

print(prediction)

prediction_label=[np.argmax(prediction)]
print(prediction_label)

if (prediction_label[0]==0):
  print('Malignant')
else:
  print('Benign')

"""Argmax function"""

my_list=[10,20,30]
index_of_the_max_value=np.argmax(my_list)
print(my_list)
print(index_of_the_max_value)

"""Saving the trained model"""

import pickle

filename = 'breast_cancer_model.sav'
pickle.dump(model, open(filename, 'wb'))

# loading the saved model
loaded_model = pickle.load(open('breast_cancer_model.sav', 'rb'))

input_data=(13.03,18.42,82.61,523.8,0.08983,0.03766,0.02562,0.02923,0.1467,0.05863,0.1839,2.342,1.17,14.16,0.004352,0.004899,0.01343,0.01164,0.02671,0.001777,13.3,22.81,84.46,545.9,0.09701,0.04619,0.04833,0.05013,0.1987,0.06169)
#change the input data to the numpy array
input_data_as_numpy_array=np.asarray(input_data)

#reshape the numpy array as we are predicting for one data point
input_data_as_reshaped=input_data_as_numpy_array.reshape(1,-1)

#strandardizing the input dta
input_data_std=scaler.transform(input_data_as_reshaped)
prediction=model.predict(input_data_std)

print(prediction)

prediction_label=[np.argmax(prediction)]
print(prediction_label)

if (prediction_label[0]==0):
  print('Malignant')
else:
  print('Benign')

for column in X.columns:
  print(column)